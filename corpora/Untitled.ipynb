{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import codecs\n",
    "import re as regex\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_sents = [line for line in\n",
    "                 codecs.open(\"test.txt\", 'r', 'utf-8').read().split(\"\\n\") if line]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(src_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_vocab(path):\n",
    "    vocab = [line.split()[0] for line in codecs.open(path, 'r', 'utf-8')]\n",
    "    word2idx = {word: idx for idx, word in enumerate(vocab)}\n",
    "    idx2word = {idx: word for idx, word in enumerate(vocab)}\n",
    "    return word2idx, idx2word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_source_vocab():\n",
    "    return load_vocab(\"src.vocab.tsv\")\n",
    "\n",
    "\n",
    "def load_target_vocab():\n",
    "    return load_vocab(\"tgt.vocab.tsv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "_WORD_SPLIT = re.compile(\"([.,!?\\\"':;)(])\")\n",
    "def basic_tokenizer(sentence, lower=True):\n",
    "    words = []\n",
    "    for space_separated_fragment in sentence.strip().split():\n",
    "        words.extend(_WORD_SPLIT.split(space_separated_fragment))\n",
    "    return [w.lower() if lower else w for w in words if w != '' and w != ' ']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_test_data(source_sents):\n",
    "    src2idx, idx2src = load_source_vocab()\n",
    "    # Index\n",
    "    x_list, sources = [], []\n",
    "    for source_sent in tqdm(source_sents, desc=\"Preparing data: \", total=len(source_sents)):\n",
    "        source_sent = basic_tokenizer(source_sent,lower=False)\n",
    "        x = [src2idx.get(word, src2idx[\"<unk>\"]) for word in source_sent]\n",
    "        x_list.append(np.array(x))\n",
    "        sources.append(source_sent)\n",
    "        print(source_sent)\n",
    "\n",
    "    max_infer_len = np.max([len(x) for x in x_list])\n",
    "    X = np.zeros([len(x_list), max_infer_len], np.int32)\n",
    "    actual_lengths = []\n",
    "    for i, x in tqdm(enumerate(x_list), desc=\"Padding: \", total=len(x_list)):\n",
    "        actual_lengths.append(len(x))\n",
    "        X[i] = np.lib.pad(x, [0, max_infer_len - len(x)], 'constant', constant_values=(0, 0))\n",
    "\n",
    "    return X, sources, actual_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Preparing data:   0%|                                                                            | 0/7 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Bo', 'phim', 'lan', 'dau', 'duoc', 'cong', 'chieu', 'tai', 'lien', 'hoan', 'phim', 'Rome', '2007', 'va', 'sau', 'do', 'duoc', 'chieu', 'o', 'Fairbanks', ',', 'Alaska', 'ngay', '21', 'thang', '9', 'nam', '2007', '.']\n",
      "['Troi', 'mua', 'mua', 'ao', 'mua']\n",
      "['me', 'bao', 'em', 'dam', 'dang']\n",
      "['cái', 'nha', 'rung', 'lac', 'Vi', 'anh', 'em', 'ba', 'con', 'nhay', 'ram', 'ram', 'nhu', 'muon', 'sap']\n",
      "['anh', 'ngu', 'chua', ',', 'sao', 'anh', 'khong', 'den', ',', 'em', 'om', '2', 'thang', 'nay', 'met', 'lam']\n",
      "['em', 'boc', 'cut', 'lon', 'lam', 'gi', 'The']\n",
      "['Hom', 'nay', ',', 'bao', 'cao', 'cua', 'Counterpoint', 'Research', 'cho', 'thay', ',', 'trong', 'nam', '2018', 'Apple', 'da', 'ban', 'duoc', 'khoang', '35', 'trieu', 'cap', 'tai', 'nghe', 'khong', 'day', 'AirPods', '.', 'Theo', 'hang', 'phan', 'tich', 'nay', ',', 'AirPods', 'hien', 'la', 'tai', 'nghe', 'khong', 'day', 'pho', 'bien', 'nhat', '.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preparing data: 100%|███████████████████████████████████████████████████████████████████| 7/7 [00:00<00:00, 999.77it/s]\n",
      "Padding: 100%|███████████████████████████████████████████████████████████████████████████████████| 7/7 [00:00<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "X, Sources, actual_lengths = create_test_data(src_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Bo',\n",
       " 'phim',\n",
       " 'lan',\n",
       " 'dau',\n",
       " 'duoc',\n",
       " 'cong',\n",
       " 'chieu',\n",
       " 'tai',\n",
       " 'lien',\n",
       " 'hoan',\n",
       " 'phim',\n",
       " 'Rome',\n",
       " '2007',\n",
       " 'va',\n",
       " 'sau',\n",
       " 'do',\n",
       " 'duoc',\n",
       " 'chieu',\n",
       " 'o',\n",
       " 'Fairbanks',\n",
       " ',',\n",
       " 'Alaska',\n",
       " 'ngay',\n",
       " '21',\n",
       " 'thang',\n",
       " '9',\n",
       " 'nam',\n",
       " '2007',\n",
       " '.']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "basic_tokenizer(src_sents[0], lower=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Bo',\n",
       "  'phim',\n",
       "  'lan',\n",
       "  'dau',\n",
       "  'duoc',\n",
       "  'cong',\n",
       "  'chieu',\n",
       "  'tai',\n",
       "  'lien',\n",
       "  'hoan',\n",
       "  'phim',\n",
       "  'Rome',\n",
       "  '2007',\n",
       "  'va',\n",
       "  'sau',\n",
       "  'do',\n",
       "  'duoc',\n",
       "  'chieu',\n",
       "  'o',\n",
       "  'Fairbanks',\n",
       "  ',',\n",
       "  'Alaska',\n",
       "  'ngay',\n",
       "  '21',\n",
       "  'thang',\n",
       "  '9',\n",
       "  'nam',\n",
       "  '2007',\n",
       "  '.'],\n",
       " ['Troi', 'mua', 'mua', 'ao', 'mua'],\n",
       " ['me', 'bao', 'em', 'dam', 'dang'],\n",
       " ['cái',\n",
       "  'nha',\n",
       "  'rung',\n",
       "  'lac',\n",
       "  'Vi',\n",
       "  'anh',\n",
       "  'em',\n",
       "  'ba',\n",
       "  'con',\n",
       "  'nhay',\n",
       "  'ram',\n",
       "  'ram',\n",
       "  'nhu',\n",
       "  'muon',\n",
       "  'sap'],\n",
       " ['anh',\n",
       "  'ngu',\n",
       "  'chua',\n",
       "  ',',\n",
       "  'sao',\n",
       "  'anh',\n",
       "  'khong',\n",
       "  'den',\n",
       "  ',',\n",
       "  'em',\n",
       "  'om',\n",
       "  '2',\n",
       "  'thang',\n",
       "  'nay',\n",
       "  'met',\n",
       "  'lam'],\n",
       " ['em', 'boc', 'cut', 'lon', 'lam', 'gi', 'The'],\n",
       " ['Hom',\n",
       "  'nay',\n",
       "  ',',\n",
       "  'bao',\n",
       "  'cao',\n",
       "  'cua',\n",
       "  'Counterpoint',\n",
       "  'Research',\n",
       "  'cho',\n",
       "  'thay',\n",
       "  ',',\n",
       "  'trong',\n",
       "  'nam',\n",
       "  '2018',\n",
       "  'Apple',\n",
       "  'da',\n",
       "  'ban',\n",
       "  'duoc',\n",
       "  'khoang',\n",
       "  '35',\n",
       "  'trieu',\n",
       "  'cap',\n",
       "  'tai',\n",
       "  'nghe',\n",
       "  'khong',\n",
       "  'day',\n",
       "  'AirPods',\n",
       "  '.',\n",
       "  'Theo',\n",
       "  'hang',\n",
       "  'phan',\n",
       "  'tich',\n",
       "  'nay',\n",
       "  ',',\n",
       "  'AirPods',\n",
       "  'hien',\n",
       "  'la',\n",
       "  'tai',\n",
       "  'nghe',\n",
       "  'khong',\n",
       "  'day',\n",
       "  'pho',\n",
       "  'bien',\n",
       "  'nhat',\n",
       "  '.']]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1525,   39,    6,   98,  111,  118,    1,    1,   78,   65,    6,\n",
       "         60,   25,    1,    1,   97,   42,   13,  385,    1,  267,  215,\n",
       "         16,  272,   93,  218,    1,    4,  592,  249,  191,  567,   39,\n",
       "          6,    1,  213,   50,   16,  272,   93,  218,  679,  654,  287,\n",
       "          4])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "src2idx, idx2src = load_source_vocab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1525"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src2idx[\"Hom\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
